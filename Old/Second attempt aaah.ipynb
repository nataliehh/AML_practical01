{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e516e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import L1L2, L2\n",
    "\n",
    "# SMOTE\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "\n",
    "\n",
    "# (Kernel) regularization: https://stats.stackexchange.com/questions/383310/what-is-the-difference-between-kernel-bias-and-activity-regulizers-and-when-t\n",
    "# https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "\n",
    "# Adaboost:\n",
    "# https://stackoverflow.com/questions/39063676/how-to-boost-a-keras-based-neural-network-using-adaboost\n",
    "\n",
    "# Xgboost:\n",
    "# https://towardsdatascience.com/cage-match-xgboost-vs-keras-deep-learning-a8bb2f69a9ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b61803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty values\n",
    "def fill_empty(df):\n",
    "    # Get which columns have empty values\n",
    "    num_nulls = df.isnull().sum()\n",
    "    has_nulls = num_nulls[num_nulls > 0].to_frame()\n",
    "    \n",
    "    \n",
    "    # Get which columns potentially are numerical based on the number of values\n",
    "    num_values = df[has_nulls.index].apply(lambda x: len(x.value_counts()))\n",
    "    categorical_cols = num_values[num_values<=10].to_frame().index\n",
    "    \n",
    "    # Get the columns with outliers\n",
    "    has_outliers = []\n",
    "\n",
    "    # For each column with nulls/nas\n",
    "    for col in has_nulls.index:\n",
    "        # Check if there's outliers in the column\n",
    "        outliers = (np.abs(stats.zscore(df[col].dropna())) >= 3).any()\n",
    "        has_outliers += [outliers]\n",
    "        \n",
    "    # Store which columns have outliers\n",
    "    has_nulls['has_outliers'] = has_outliers  \n",
    "    has_nulls = has_nulls.reset_index()\n",
    "    \n",
    "    nulls_and_outliers = has_nulls[has_nulls.has_outliers == True]['index']\n",
    "    nulls_no_outliers = has_nulls[has_nulls.has_outliers == False]['index']\n",
    "\n",
    "    # Fill empty values appropriately (mean for non-cat., no outliers; median for non-cat., outliers; mode for cat.)\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode())\n",
    "    df[nulls_no_outliers] = df[nulls_no_outliers].fillna(df[nulls_no_outliers].mean())\n",
    "    df[nulls_and_outliers] = df[nulls_and_outliers].fillna(df[nulls_and_outliers].median())\n",
    "#     df = df.fillna(0)\n",
    "    return df, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689f2a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    1032\n",
      "1     314\n",
      "0     247\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "df = pd.read_csv('./train_data.csv')\n",
    "\n",
    "print(df.target.value_counts())\n",
    "\n",
    "# Make boolean columns into integer columns\n",
    "df.replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "# Split the data into target labels y and features X\n",
    "num_train = df.select_dtypes(include=np.number)\n",
    "train_y = df.target\n",
    "train_X = df.drop(['target'], axis = 1)\n",
    "\n",
    "# Drop columns with too many nulls\n",
    "# num_nulls = df.isnull().sum() \n",
    "# rows = train_X.shape[0]\n",
    "# too_many_nulls = num_nulls[(num_nulls > rows/1.1)].to_frame()\n",
    "# drop_cols = list(too_many_nulls.index)\n",
    "# train_X.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# Fill empty values in the train data   \n",
    "train_X, categorical_cols = fill_empty(train_X) #train_X.fillna(train_X.mean())\n",
    "\n",
    "# Custom z-score method because scipy caused NaN problems in some columns\n",
    "def z_score(arr):\n",
    "    mean = np.mean(arr, axis = 0)\n",
    "    std = np.std(arr, axis = 0)\n",
    "    std[std == 0] = 1\n",
    "    return (arr-mean)/std\n",
    "\n",
    "col_idx = []\n",
    "for col in categorical_cols:\n",
    "    col_idx.append(train_X.columns.get_loc(col))\n",
    "\n",
    "# Normalize the data using z-score standardization\n",
    "train_X = z_score(train_X.to_numpy(dtype = np.float64))\n",
    "train_df = pd.DataFrame(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ed5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample a little bit for the target values that are underrepresented (e.g. target=0, target=1)\n",
    "smote_nc = SMOTENC(categorical_features=col_idx, random_state=0, sampling_strategy = {1:450, 0:350}) # \n",
    "train_X, train_y = smote_nc.fit_resample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc11bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn y into categorical variable\n",
    "y = to_categorical(train_y, num_classes = 3)\n",
    "input_dim = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49db3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = pd.DataFrame(train_X)\n",
    "feature_names = [f\"feature {i}\" for i in range(train_X.shape[1])]\n",
    "forest = RandomForestClassifier(random_state=0, criterion = 'entropy')\n",
    "col_importances = forest.fit(train_X, train_y).feature_importances_\n",
    "\n",
    "columns = train_X.columns.to_frame()\n",
    "columns['importance'] = col_importances\n",
    "columns.sort_values(by = 'importance', ascending = False)\n",
    "important_cols_names = columns[columns.importance>=columns.importance.median()]\n",
    "important_cols = important_cols_names.index\n",
    "# train_df = train_X[important_cols]\n",
    "\n",
    "train_X = train_X[important_cols].to_numpy(dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a7a9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, dropout = 0.5, lr = 1e-2):\n",
    "    initializer = keras.initializers.HeNormal()\n",
    "    regularizer = L2(0.01)\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # Relu layers: HE15 kernel initializer\n",
    "    # Maybe kernel regularization?\n",
    "    model.add(Dense(256, input_dim=input_dim, activation='relu', kernel_initializer = initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer = initializer, kernel_regularizer = regularizer)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer = initializer, kernel_regularizer = regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # keras.metrics.Accuracy(), keras.metrics.Precision(), keras.metrics.Recall()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=lr, momentum = 0.9, nesterov=True), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def avg(arr):\n",
    "    return sum(arr)/len(arr)\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cc5ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# class CategoricalClassifier():\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def fit(self, X, y, batch_size = 32, n_epochs = 40):\n",
    "#         self.classifier = create_model(X.shape[1])\n",
    "\n",
    "#         self.classifier.fit(X, y, batch_size = batch_size, epochs = n_epochs, verbose = 0)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         return np.argmax(self.classifier.predict(X), axis = -1)\n",
    "\n",
    "#     def predict(self, X, y=None):\n",
    "#         return self.transform(X)\n",
    "# Make a train-validation split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.1, shuffle = True)\n",
    "\n",
    "# estimator = KerasClassifier(build_fn=create_model, epochs=40, batch_size=32, verbose=1)\n",
    "# boosted_ann = AdaBoostClassifier(base_estimator= estimator)\n",
    "# boosted_ann.fit(X_train, y_train)# scale your training data \n",
    "# boosted_ann.predict(rescaledX_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5b2b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 3ms/step - loss: 3.4169 - accuracy: 0.5182\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.7431 - accuracy: 0.6196\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.4921 - accuracy: 0.5910\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.0638 - accuracy: 0.6576\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.9837 - accuracy: 0.6044\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.6781 - accuracy: 0.6793\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6184 - accuracy: 0.6377\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.4002 - accuracy: 0.6793\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3707 - accuracy: 0.6596\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.1710 - accuracy: 0.7120\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1958 - accuracy: 0.6839\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0522 - accuracy: 0.7283\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1006 - accuracy: 0.6942\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9988 - accuracy: 0.7065\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0077 - accuracy: 0.7124\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9083 - accuracy: 0.7500\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9430 - accuracy: 0.7215\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8946 - accuracy: 0.7337\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9180 - accuracy: 0.7209\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8424 - accuracy: 0.7391\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8842 - accuracy: 0.7391\n",
      "6/6 [==============================] - 0s 999us/step - loss: 0.8210 - accuracy: 0.7500\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8647 - accuracy: 0.7470\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7844 - accuracy: 0.7663\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7794 - accuracy: 0.7633\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7423 - accuracy: 0.7772\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8076 - accuracy: 0.7670\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7709 - accuracy: 0.7717\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8105 - accuracy: 0.7567\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7260 - accuracy: 0.7989\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.7816 - accuracy: 0.7694\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.8207\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/47892505/dropout-rate-guidance-for-hidden-layers-in-a-convolution-neural-network\n",
    "def train_network(model, X_train, y_train, X_val, y_val, \n",
    "                  n_epochs = 10, batch_size = 32, verbose = 1, save = False, path = 'best_model'):\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_loss = 1\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Train model\n",
    "        results = model.fit(X_train, y_train, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "        # Get training loss and accuracy and append to lists\n",
    "        train_losses.append(results.history['loss'])\n",
    "        train_accs.append(results.history['accuracy'])\n",
    "\n",
    "        # Get performance (i.e. loss and accuracy) on validation set\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose = verbose)\n",
    "\n",
    "        # Append to lists\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Keep track of which epoch had the best validation accuracy and store the corresponding model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "#             if save:\n",
    "#                 model.save(path)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if save:\n",
    "                model.save(path)\n",
    "\n",
    "    # Visualise the performance of the model for the epochs\n",
    "    if verbose > 0:\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        train_loss_plt = plt.plot(train_losses) \n",
    "        train_accs_plt = plt.plot(train_accs)\n",
    "        val_loss_plt = plt.plot(val_losses)\n",
    "        val_acc_plt = plt.plot(val_accs)\n",
    "\n",
    "        # Text and legend for the plot\n",
    "        plt.legend(['train loss', 'train accuracy', 'validation loss', 'validation accuracy'],\n",
    "                  loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('Model performance')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Print the performance values (accuracies, loss) for train, validation\n",
    "        print('Train accuracy:\\t\\t\\t{:.2f}\\tTrain loss: {:.2f}'.format(train_accs[-1][0], train_losses[-1][0]))\n",
    "        print('Validation accuracy:\\t\\t{:.2f}\\tValidation loss: {:.2f}'.format(val_accs[-1], val_losses[-1]))\n",
    "        print('Best validation accuracy:\\t{:.2f} (epoch {:})'.format(best_val_acc, best_epoch))\n",
    "\n",
    "# Make a train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, y, test_size=0.1, shuffle = True)\n",
    "model = create_model(train_X.shape[1])\n",
    "train_network(model, X_train, y_train, X_val, y_val, n_epochs = 30, save = True, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acb81b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.45600911e-03 3.62062268e-03 9.94923413e-01]\n",
      " [2.12711887e-03 5.90081699e-03 9.91972089e-01]\n",
      " [9.07395128e-03 2.20946278e-02 9.68831480e-01]\n",
      " [9.34596211e-02 2.66224474e-01 6.40315890e-01]\n",
      " [2.50455160e-02 3.34315747e-02 9.41522956e-01]\n",
      " [2.54195798e-02 3.11638452e-02 9.43416536e-01]\n",
      " [1.21988937e-01 1.96874678e-01 6.81136310e-01]\n",
      " [2.66686976e-02 2.71334618e-01 7.01996684e-01]\n",
      " [2.05547418e-02 7.61334538e-01 2.18110666e-01]\n",
      " [1.99767854e-02 5.12814820e-01 4.67208356e-01]\n",
      " [7.14913756e-02 2.15444863e-01 7.13063717e-01]\n",
      " [1.76199839e-01 3.99056584e-01 4.24743593e-01]\n",
      " [9.99036729e-02 2.49544352e-01 6.50552034e-01]\n",
      " [1.10010982e-01 3.61825794e-01 5.28163195e-01]\n",
      " [2.14056104e-01 1.91294849e-01 5.94649017e-01]\n",
      " [1.11414514e-01 4.87378925e-01 4.01206642e-01]\n",
      " [5.04498668e-02 9.95576605e-02 8.49992454e-01]\n",
      " [1.25700003e-02 1.55789226e-01 8.31640840e-01]\n",
      " [4.53325082e-03 2.97957212e-02 9.65671003e-01]\n",
      " [1.68079183e-01 6.53016686e-01 1.78904101e-01]\n",
      " [3.39835376e-01 3.10455829e-01 3.49708855e-01]\n",
      " [1.19912559e-02 1.24447241e-01 8.63561511e-01]\n",
      " [3.96523345e-03 3.43928598e-02 9.61641908e-01]\n",
      " [1.85238314e-03 8.88956897e-03 9.89258111e-01]\n",
      " [5.47378091e-04 1.29963353e-03 9.98152912e-01]\n",
      " [7.70636034e-05 3.24244815e-04 9.99598682e-01]\n",
      " [4.88552421e-01 3.74118268e-01 1.37329355e-01]\n",
      " [1.33637460e-02 4.81136404e-02 9.38522637e-01]\n",
      " [6.85593188e-01 2.36229300e-01 7.81775340e-02]\n",
      " [6.02805674e-01 3.54801953e-01 4.23923954e-02]\n",
      " [7.10150123e-01 2.63437122e-01 2.64127348e-02]\n",
      " [5.00964701e-01 4.07692552e-01 9.13427770e-02]\n",
      " [6.25170588e-01 2.90195733e-01 8.46337676e-02]\n",
      " [6.48999989e-01 2.22606793e-01 1.28393188e-01]\n",
      " [6.77525818e-01 1.54729351e-01 1.67744786e-01]\n",
      " [6.07943296e-01 3.14714164e-01 7.73424879e-02]\n",
      " [6.88905656e-01 2.17914969e-01 9.31793973e-02]\n",
      " [3.74327511e-01 5.54202199e-01 7.14703500e-02]\n",
      " [4.91387576e-01 4.02244836e-01 1.06367663e-01]\n",
      " [5.59318781e-01 2.83391148e-01 1.57290071e-01]\n",
      " [5.98529935e-01 1.96744233e-01 2.04725817e-01]\n",
      " [3.69812757e-01 4.74378794e-01 1.55808419e-01]\n",
      " [4.42042887e-01 3.23346943e-01 2.34610200e-01]\n",
      " [4.57456470e-01 2.15876967e-01 3.26666623e-01]\n",
      " [4.23193067e-01 1.37441680e-01 4.39365268e-01]\n",
      " [5.38960576e-01 1.22436322e-01 3.38603050e-01]\n",
      " [4.47436541e-01 6.13198467e-02 4.91243571e-01]\n",
      " [3.78705025e-01 4.18639816e-02 5.79430938e-01]\n",
      " [5.27384698e-01 7.31968433e-02 3.99418443e-01]\n",
      " [3.34304422e-01 2.92293467e-02 6.36466205e-01]\n",
      " [1.38279527e-01 1.43768668e-01 7.17951834e-01]\n",
      " [4.75605614e-02 7.64829963e-02 8.75956416e-01]\n",
      " [4.75530662e-02 5.76523900e-01 3.75923127e-01]\n",
      " [9.14406255e-02 6.55002177e-01 2.53557175e-01]\n",
      " [1.26443371e-01 3.18787187e-01 5.54769456e-01]\n",
      " [1.31505400e-01 2.30877608e-01 6.37616992e-01]\n",
      " [2.11608738e-01 4.74739522e-02 7.40917325e-01]\n",
      " [1.03438370e-01 6.79684207e-02 8.28593194e-01]\n",
      " [1.64147443e-03 7.29390467e-03 9.91064668e-01]\n",
      " [9.12912011e-01 2.12736446e-02 6.58143461e-02]\n",
      " [1.87224895e-01 6.23842515e-02 7.50390828e-01]\n",
      " [5.73322117e-01 1.83616444e-01 2.43061498e-01]\n",
      " [8.58818829e-01 4.51938584e-02 9.59873274e-02]\n",
      " [7.46102184e-02 4.75378381e-03 9.20635998e-01]\n",
      " [5.78968227e-03 2.59375223e-03 9.91616547e-01]\n",
      " [7.89728984e-02 3.02683383e-01 6.18343711e-01]\n",
      " [1.90427050e-01 2.22968549e-01 5.86604476e-01]\n",
      " [2.96716690e-01 2.58382976e-01 4.44900423e-01]\n",
      " [3.71393710e-02 2.09620669e-01 7.53239930e-01]\n",
      " [4.71725076e-01 9.32507813e-02 4.35024172e-01]\n",
      " [4.63866144e-01 1.26887411e-01 4.09246475e-01]\n",
      " [2.43682275e-03 2.98251281e-03 9.94580686e-01]\n",
      " [4.48366889e-04 1.21899904e-03 9.98332679e-01]\n",
      " [1.14473365e-02 2.43364032e-02 9.64216232e-01]\n",
      " [2.25335173e-03 6.79740123e-03 9.90949214e-01]\n",
      " [2.06828918e-02 2.42745690e-02 9.55042481e-01]\n",
      " [3.91709916e-02 1.60579830e-01 8.00249159e-01]\n",
      " [3.39393434e-03 1.66052468e-02 9.80000794e-01]\n",
      " [2.95350747e-03 8.08111951e-03 9.88965392e-01]\n",
      " [4.89983737e-01 2.74104308e-02 4.82605904e-01]\n",
      " [2.24595249e-01 2.95539889e-02 7.45850742e-01]\n",
      " [3.69098186e-01 9.43470523e-02 5.36554694e-01]\n",
      " [6.22605264e-01 7.34753758e-02 3.03919345e-01]\n",
      " [8.75769973e-01 4.66797277e-02 7.75503591e-02]\n",
      " [4.05195087e-01 2.85192311e-01 3.09612542e-01]\n",
      " [2.08640307e-01 6.30681098e-01 1.60678580e-01]\n",
      " [5.14566243e-01 2.13917494e-01 2.71516263e-01]\n",
      " [6.40361488e-01 1.68219730e-01 1.91418812e-01]\n",
      " [6.58264160e-01 1.28058270e-01 2.13677600e-01]\n",
      " [3.79333138e-01 2.26534024e-01 3.94132823e-01]\n",
      " [4.09777403e-01 1.47812098e-01 4.42410529e-01]\n",
      " [5.72396107e-02 3.37124057e-02 9.09048021e-01]\n",
      " [2.61792942e-04 9.32902738e-04 9.98805285e-01]\n",
      " [2.46072877e-02 3.11473496e-02 9.44245398e-01]\n",
      " [1.72217041e-02 3.71739902e-02 9.45604324e-01]\n",
      " [1.41150374e-02 3.54145169e-02 9.50470448e-01]\n",
      " [9.68311541e-03 1.73825473e-02 9.72934365e-01]\n",
      " [1.91077217e-03 1.22066122e-02 9.85882640e-01]\n",
      " [1.68184016e-03 9.75240301e-03 9.88565743e-01]\n",
      " [7.37246498e-03 6.19370490e-03 9.86433864e-01]\n",
      " [3.00722308e-02 1.13241807e-01 8.56685936e-01]\n",
      " [2.02473477e-02 3.66465412e-02 9.43106115e-01]\n",
      " [5.49736293e-03 1.21621396e-02 9.82340455e-01]\n",
      " [4.47393730e-02 1.06286392e-01 8.48974228e-01]\n",
      " [3.40068666e-03 1.56802230e-03 9.95031238e-01]\n",
      " [6.67969417e-03 4.39869147e-03 9.88921642e-01]\n",
      " [3.17948982e-02 6.27707643e-03 9.61928070e-01]\n",
      " [1.65068991e-02 5.19630238e-02 9.31530058e-01]\n",
      " [2.01473404e-02 4.12100740e-02 9.38642561e-01]\n",
      " [1.06065738e-04 2.01399715e-04 9.99692559e-01]\n",
      " [1.58251423e-04 1.58250812e-04 9.99683499e-01]\n",
      " [2.81362627e-02 2.44520400e-02 9.47411716e-01]\n",
      " [2.80958309e-04 4.31335269e-04 9.99287784e-01]\n",
      " [2.84173992e-02 1.94811508e-01 7.76771069e-01]\n",
      " [1.23890843e-02 9.26437508e-03 9.78346527e-01]\n",
      " [2.84063220e-01 6.13920763e-02 6.54544711e-01]\n",
      " [7.74707913e-01 1.37213632e-01 8.80784020e-02]\n",
      " [1.73654919e-03 7.31115183e-03 9.90952253e-01]\n",
      " [2.18816247e-04 5.66392555e-04 9.99214768e-01]\n",
      " [4.56234120e-04 1.33181503e-03 9.98211980e-01]\n",
      " [7.45939314e-01 1.08069710e-01 1.45990983e-01]\n",
      " [4.96866591e-02 8.21227282e-02 8.68190646e-01]\n",
      " [1.19508713e-01 2.62895059e-02 8.54201734e-01]\n",
      " [1.45845965e-01 3.01632192e-02 8.23990822e-01]\n",
      " [3.62212062e-01 2.71138698e-02 6.10674024e-01]\n",
      " [2.18041837e-01 4.63627018e-02 7.35595405e-01]\n",
      " [5.50304726e-02 3.26755792e-01 6.18213773e-01]\n",
      " [1.00181662e-02 2.26607732e-02 9.67321038e-01]\n",
      " [7.34888464e-02 4.40032244e-01 4.86478925e-01]\n",
      " [1.41060561e-01 4.79274482e-01 3.79664958e-01]\n",
      " [1.55701876e-01 1.00009870e-02 8.34297180e-01]\n",
      " [4.06074166e-01 5.66863157e-02 5.37239492e-01]\n",
      " [8.60310912e-01 1.14375934e-01 2.53130849e-02]\n",
      " [2.19222694e-03 5.98365767e-03 9.91824090e-01]\n",
      " [2.16845074e-03 9.96531337e-04 9.96835053e-01]\n",
      " [1.80598810e-01 3.64342481e-01 4.55058783e-01]\n",
      " [2.95918137e-02 4.29808274e-02 9.27427351e-01]\n",
      " [4.90792468e-03 1.09510301e-02 9.84141111e-01]\n",
      " [1.30806744e-01 3.42430584e-02 8.34950209e-01]\n",
      " [4.62946808e-03 6.38114288e-03 9.88989413e-01]\n",
      " [5.40251890e-03 7.73542700e-03 9.86862063e-01]\n",
      " [6.82270620e-04 9.97348980e-05 9.99217987e-01]\n",
      " [1.19234482e-03 4.31226753e-03 9.94495332e-01]\n",
      " [1.44757086e-03 2.06046831e-03 9.96491969e-01]\n",
      " [9.50771570e-03 6.20761607e-03 9.84284639e-01]\n",
      " [5.21522597e-04 1.55924435e-03 9.97919261e-01]\n",
      " [3.74523625e-02 1.96553335e-01 7.65994370e-01]\n",
      " [1.20721746e-03 3.45250778e-03 9.95340228e-01]\n",
      " [6.52936473e-02 3.47073585e-01 5.87632775e-01]\n",
      " [4.29212928e-01 2.81299323e-01 2.89487690e-01]\n",
      " [1.81850731e-01 7.36755371e-01 8.13939273e-02]\n",
      " [2.38948822e-01 6.91060662e-01 6.99905008e-02]\n",
      " [2.57171214e-01 5.19092739e-01 2.23736048e-01]\n",
      " [2.24699259e-01 5.87367058e-01 1.87933788e-01]\n",
      " [3.09082419e-01 4.06430721e-01 2.84486920e-01]\n",
      " [5.30089617e-01 1.25939146e-01 3.43971163e-01]\n",
      " [3.15196300e-03 2.36263010e-03 9.94485438e-01]\n",
      " [3.69951385e-03 4.85659298e-03 9.91443932e-01]\n",
      " [4.46187332e-03 2.88236700e-03 9.92655754e-01]\n",
      " [6.13188036e-02 9.64535251e-02 8.42227697e-01]\n",
      " [2.09979355e-01 2.11777136e-01 5.78243554e-01]\n",
      " [8.26827958e-02 1.00108832e-02 9.07306373e-01]\n",
      " [1.40818497e-02 4.09536622e-02 9.44964528e-01]\n",
      " [2.87779588e-02 8.81715640e-02 8.83050501e-01]\n",
      " [6.64239749e-02 1.67758390e-01 7.65817642e-01]\n",
      " [3.12939256e-01 5.50271690e-01 1.36788979e-01]\n",
      " [3.47810909e-02 1.15773708e-01 8.49445224e-01]\n",
      " [2.87587903e-02 7.80266523e-02 8.93214524e-01]\n",
      " [1.79349482e-02 5.50672375e-02 9.26997781e-01]\n",
      " [3.68237612e-03 1.33996746e-02 9.82917964e-01]\n",
      " [1.01989936e-02 1.35927247e-02 9.76208329e-01]\n",
      " [5.53304376e-03 7.12589594e-03 9.87341106e-01]\n",
      " [3.48681659e-02 4.14966904e-02 9.23635185e-01]\n",
      " [1.37102334e-02 4.28059995e-02 9.43483829e-01]\n",
      " [8.46602581e-03 2.40044743e-02 9.67529416e-01]\n",
      " [1.31718861e-02 3.65899764e-02 9.50238109e-01]\n",
      " [8.60535800e-02 3.71432975e-02 8.76803160e-01]\n",
      " [1.11791119e-01 1.95561796e-01 6.92647099e-01]\n",
      " [1.25058547e-01 7.36861885e-01 1.38079599e-01]\n",
      " [1.77961066e-01 6.31683350e-01 1.90355495e-01]\n",
      " [1.82173193e-01 5.44416428e-01 2.73410380e-01]\n",
      " [1.03826448e-01 6.85364008e-01 2.10809544e-01]\n",
      " [1.20512635e-01 5.58348656e-01 3.21138740e-01]\n",
      " [8.61494467e-02 4.68868762e-01 4.44981813e-01]\n",
      " [9.60428640e-02 3.65604073e-01 5.38353086e-01]\n",
      " [8.92084837e-02 5.91736495e-01 3.19055051e-01]\n",
      " [8.84702355e-02 4.90635723e-01 4.20894027e-01]\n",
      " [3.26866537e-01 6.35451138e-01 3.76823097e-02]]\n",
      "[2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 0 2 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 2 0 2 2 0 2 2 2 1 1 2 2 2 2 2 0 2 0 0 2 2 2 2 2 2 0 0 2 2 2\n",
      " 2 2 2 2 2 0 2 2 0 0 0 1 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 0 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 2\n",
      " 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create prediction dataset\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('best_model')\n",
    "naming_cols = ['opportunity_id', 'current_date_day', 'current_date_month', 'current_date_year']\n",
    "test_df = pd.read_csv('./data2tal_testupload.csv')\n",
    "index_names = pd.read_csv('./check_upload_kaggle.csv')\n",
    "# test_df.drop(drop_cols, axis=1, inplace=True)\n",
    "test_df, cat_ = fill_empty(test_df)\n",
    "\n",
    "test_df.columns = list(range(test_df.shape[1]))\n",
    "test_df = test_df[important_cols]\n",
    "train_X_df = pd.DataFrame(train_X)\n",
    "train_X_df.columns = list(range(train_X_df.shape[1]))\n",
    "test_df.columns = list(range(train_X_df.shape[1]))\n",
    "test_df = test_df.fillna(-1) #train_X_df.mean()\n",
    "test_df = test_df.to_numpy(dtype = np.float64)\n",
    "test_X = z_score(test_df)\n",
    "pred = model.predict(test_X)\n",
    "print(pred)\n",
    "pred_target = np.argmax(pred, axis = 1)\n",
    "print(pred_target)\n",
    "df_pred = pd.DataFrame([])\n",
    "df_pred['index'] = index_names['index']\n",
    "df_pred['target'] = pred_target\n",
    "\n",
    "df_pred.to_csv('./prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696408ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n"
     ]
    }
   ],
   "source": [
    "def get_rows_with_nulls(df):\n",
    "    num_nulls = df.isna().sum()\n",
    "    return num_nulls[num_nulls > 0]\n",
    "\n",
    "test_data = pd.read_csv('./data2tal_testupload.csv')\n",
    "train_data = pd.read_csv('./train_data.csv')\n",
    "\n",
    "test_nulls = get_rows_with_nulls(test_data)\n",
    "test_rows = test_data.shape[0]\n",
    "fully_empty_test = list(test_nulls[test_nulls >= test_rows].index)\n",
    "\n",
    "train_nulls = get_rows_with_nulls(train_data)\n",
    "train_rows = train_data.shape[0]\n",
    "mostly_empty_train = list(train_nulls[train_nulls>= train_rows/1.15].index)\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "intersect = intersection(mostly_empty_train, fully_empty_test)\n",
    "print(len(intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ec3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 506     188 (filled)\n",
    "# 896     188 (filled)\n",
    "# 1754    188\n",
    "# 1904    188\n",
    "# 2202    188\n",
    "# 2392    188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93934b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# df = pd.read_csv('./train_data.csv')\n",
    "\n",
    "# print(df.target.value_counts())\n",
    "\n",
    "# # Make boolean columns into integer columns\n",
    "# df.replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "# # Split the data into target labels y and features X\n",
    "# num_train = df.select_dtypes(include=np.number)\n",
    "# # train_y = df.target\n",
    "# # train_X = df.drop(['target'], axis = 1)\n",
    "# np_df = df.to_numpy()\n",
    "\n",
    "# imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "# imp.fit(np_df)\n",
    "\n",
    "# # the model learns that the second feature is double the first\n",
    "# print(np.round(imp.transform(np_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a77678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
